getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[3, 4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=0.75, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
working on results/MEMIT/run_002/{}_edits-case_{}.json
about to start 73
started 73
we are in apply memit to model
copied weights
inside execute_memit
deepcopied requests
MEMIT request sample: [Lee Alvin DuBridge's area of work is] -> [ diplomat]
MEMIT request sample: [John Henry Poynting's domain of activity is] -> [ mathematics]
MEMIT request sample: [Yi-Fu Tuan's expertise is] -> [ singing]
MEMIT request sample: [Michel Chasles's domain of work is] -> [ physics]
MEMIT request sample: [Onufri works in the field of] -> [ astronomy]
MEMIT request sample: [Jonathan Haidt works in the field of] -> [ geometry]
MEMIT request sample: [A Thousand Plateaus's domain of activity is] -> [ physics]
MEMIT request sample: [The expertise of The Astronomical Journal is] -> [ algebra]
MEMIT request sample: [suicide attack specializes in] -> [ physiology]
MEMIT request sample: [The domain of activity of Georg Ernst Stahl is] -> [ mathematics]
Cached context templates [['{}'], ['The present invention relates to a method for preparing a. {}', 'Therefore, it would be highly desirable to provide a. {}', 'Because the last time I was on a plane,. {}', "I'm trying to get my head around this.. {}", "You can't have a conversation with someone if you. {}"]]


LAYER 3

Writing 352 key/value pair(s) into layer 3
z error 58.07084655761719
Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.3.mlp.fc_out.
orig norm 106.17860412597656
upd norm 16.54575885608934


LAYER 4

Writing 352 key/value pair(s) into layer 4
z error 53.522300720214844
Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.4.mlp.fc_out.
orig norm 108.45027923583984
upd norm 8.883604490247949


LAYER 5

Writing 352 key/value pair(s) into layer 5
z error 49.72978210449219
Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.5.mlp.fc_out.
orig norm 110.7964859008789
upd norm 10.39539020555038


LAYER 6

Writing 352 key/value pair(s) into layer 6
z error 45.25688552856445
Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.6.mlp.fc_out.
orig norm 113.19039916992188
upd norm 12.616736044018063


LAYER 7

Writing 352 key/value pair(s) into layer 7
z error 40.467872619628906
Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.7.mlp.fc_out.
orig norm 117.4111328125
upd norm 15.769533617925566


LAYER 8

Writing 352 key/value pair(s) into layer 8
z error 36.133567810058594
Retrieving covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.8.mlp.fc_out.
orig norm 119.0634765625
upd norm 25.68738228014495
Deltas successfully computed for ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']
executed memit
New weights successfully inserted into ['transformer.h.3.mlp.fc_out.weight', 'transformer.h.4.mlp.fc_out.weight', 'transformer.h.5.mlp.fc_out.weight', 'transformer.h.6.mlp.fc_out.weight', 'transformer.h.7.mlp.fc_out.weight', 'transformer.h.8.mlp.fc_out.weight']
Execution took 1013.1484034061432
about to evaluate
