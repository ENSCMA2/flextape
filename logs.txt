getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_000/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
Execution took 14359.546903371811
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_000/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
Execution took 1024.6458678245544
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_000/{}_edits-case_{}.json
about to start 0
started 0
Execution took 1297.0507447719574
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_000/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P103_layer_{}_clamp_{}_case_{}.npz
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
Execution took 923.8927621841431
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P21_P101_layer_{}_clamp_{}_case_{}.npz
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P21_P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_000/{}_edits-case_{}.json
about to start 21920
started 21920
Execution took 4472.587132692337
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P27_P101_layer_{}_clamp_{}_case_{}.npz
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P27_P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 23061
started 23061
Execution took 5109.439035654068
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P19_P101_layer_{}_clamp_{}_case_{}.npz
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P19_P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 22775
started 22775
Execution took 4325.3148465156555
about to evaluate
getting started
starting main
Instantiating model
created model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_NONE/P101_layer_{}_clamp_{}_case_{}.npz
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P21_P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 21920
started 21920
Execution took 288.1748151779175
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P27_P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P19_P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P101_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 73
started 73
Execution took 3691.036459684372
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/meta-llama_Llama-2-7b-chat-hf_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/llamac/MEMIT/run_001/{}_edits-case_{}.json
about to start 0
started 0
Execution took 944.9282863140106
about to evaluate
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P101_P21_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 22210
started 22210
Execution took 4288.849545240402
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P27_P19_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 23904
started 23904
Execution took 4126.420791149139
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P21_P101_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 21920
started 21920
Execution took 3873.119710922241
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P27_P101_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 23061
started 23061
Execution took 4143.006308555603
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P27_P21_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 23622
started 23622
Execution took 4111.898293495178
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P27_P19_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P101_P27_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 23343
started 23343
Execution took 4275.764112234116
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P19_P21_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 22489
started 22489
Execution took 4175.25985956192
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P19_P101_layer_{}_clamp_{}_case_{}.npz
working on pair/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 22775
started 22775
Execution took 4154.34649014473
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P21_P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 21920
started 21920
Execution took 321.0517008304596
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P27_P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 23061
started 23061
Execution took 311.9726996421814
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P19_P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 22775
started 22775
Execution took 301.63964676856995
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 73
started 73
Execution took 5049.1767909526825
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-Instruct-v0.2_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/mistral/MEMIT/run_000/{}_edits-case_{}.json
about to start 0
started 0
Execution took 11346.651837825775
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P101_P21_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 22210
started 22210
Execution took 4300.877533197403
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P27_P19_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 23904
started 23904
Execution took 4158.936620950699
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P21_P101_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 21920
started 21920
Execution took 4018.955067873001
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P27_P101_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 23061
started 23061
Execution took 4154.705704450607
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P27_P21_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 23622
started 23622
Execution took 4176.877424955368
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P27_P19_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P101_P27_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 23343
started 23343
Execution took 4290.060573816299
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P19_P21_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 22489
started 22489
Execution took 4177.920533895493
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P19_P101_layer_{}_clamp_{}_case_{}.npz
working on pair/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 22775
started 22775
Execution took 4170.575404882431
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P21_P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 21920
started 21920
Execution took 322.3486852645874
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P27_P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 23061
started 23061
Execution took 313.08674907684326
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P19_P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 22775
started 22775
Execution took 303.01149129867554
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P101_layer_{}_clamp_{}_case_{}.npz
working on results/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 73
started 73
Execution took 5193.77911400795
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
Loading dataset, attribute snippets, tf-idf data
Will load cache from kvs/mistralai_Mistral-7B-v0.1_MEMIT/P103_layer_{}_clamp_{}_case_{}.npz
working on results/mistralb/MEMIT/run_000/{}_edits-case_{}.json
about to start 0
started 0
Execution took 12088.439758777618
about to evaluate
getting started
starting main
Executing MEMIT with parameters MEMITHyperParams(layers=[4, 5, 6, 7, 8], layer_selection='all', fact_token='subject_last', v_num_grad_steps=25, v_lr=0.5, v_loss_layer=31, v_weight_decay=0.001, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, mom2_update_weight=15000, rewrite_module_tmp='model.layers.{}.mlp.down_proj', layer_module_tmp='model.layers.{}', mlp_module_tmp='model.layers.{}.mlp', attn_module_tmp='model.layers.{}.self_attn', ln_f_module='model.norm', lm_head_module='lm_head', stats_dir='./data/stats', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')
Instantiating model
